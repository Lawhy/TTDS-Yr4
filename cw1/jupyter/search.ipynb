{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import deque, OrderedDict\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search:\n",
    "    \"\"\"A class that implements:\n",
    "            1. Query Parser (using the *Shunting Yard Algorithm*)\n",
    "            2. Singleton Search (using the *Linear Merge Algorithm*):\n",
    "                (a) Single-term search;\n",
    "                (b) Proximity search (two terms);\n",
    "                (c) Phrasal search (two terms).\n",
    "            3. Boolean Search (AND, OR, NOT)\n",
    "            4.   \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, index_path, stemmer):\n",
    "        with open(index_path, 'rb') as f:\n",
    "            self.index = pickle.load(f)\n",
    "        self.stemmer = stemmer\n",
    "        doc_ids = []\n",
    "        for term in self.index.keys():\n",
    "            doc_ids += list(self.index[term].keys())\n",
    "        self.doc_ids = set(doc_ids)    # important for NOT search\n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def shunting_yard(infix_tokens):\n",
    "        \"\"\"implementation of the Shunting Yard algorithm:\n",
    "           convert infix expression to postfix expression \n",
    "\n",
    "        Args:\n",
    "            infix_tokens (str): the infix expression as a list of tokens\n",
    "\n",
    "        Returns:\n",
    "            postfix_tokens (list): the corresponding postfix expression as a list of tokens\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # define precedences\n",
    "        precedence = {'NOT': 3, 'AND': 2, 'OR': 1, '(': 0, ')': 0}   \n",
    "\n",
    "        # declare data strucures\n",
    "        output = []\n",
    "        operator_stack = []  # .pop() takes the last element out of the list\n",
    "                             # operators stored always in ascending order of precedence\n",
    "                             # because otherwise invoke .pop()\n",
    "\n",
    "        # while there are tokens to be read\n",
    "        for token in infix_tokens:\n",
    "\n",
    "            # if left bracket\n",
    "            if token == '(':\n",
    "                operator_stack.append(token)\n",
    "\n",
    "            # if right bracket, pop all operators from operator stack onto output until we hit a left bracket\n",
    "            # (the right bracket always matches the nearest left bracket)\n",
    "            elif token == ')':\n",
    "                operator = operator_stack.pop()\n",
    "                while not operator == '(':\n",
    "                    # this means the operators must be used within the parenthesis\n",
    "                    # note that the output discards the parenthesis because unneeded\n",
    "                    output.append(operator)\n",
    "                    operator = operator_stack.pop()\n",
    "\n",
    "            # if operator, pop operators from operator stack to queue if they are of higher precedence than the current token\n",
    "            elif token in precedence.keys():\n",
    "                # if operator stack is not empty\n",
    "                if operator_stack:\n",
    "                    # check the precedence of the last operator (most recently added), \n",
    "                    # i.e. the most precedent operator in the stack\n",
    "                    last_operator = operator_stack[-1]              \n",
    "                    # while stack not empty and current token of lower precedence than ops in the stack \n",
    "                    # (we must process the higher-precedence ops before adding a lower one)\n",
    "                    while (operator_stack and precedence[last_operator] >= precedence[token]):\n",
    "                        output.append(operator_stack.pop())\n",
    "                        if (operator_stack):\n",
    "                            current_operator = operator_stack[-1]\n",
    "\n",
    "                operator_stack.append(token) # add token to stack\n",
    "\n",
    "            # else if operands, add to output list\n",
    "            else:\n",
    "                output.append(token.lower())\n",
    "\n",
    "        # while there are still operators on the stack, pop them into the queue\n",
    "        while (operator_stack):\n",
    "            output.append(operator_stack.pop())\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_query(query):\n",
    "        \"\"\"read the query and split into tokens.\n",
    "           type of queries:\n",
    "               1. Boolean Query (AND, OR, NOT)\n",
    "               2. Phrasal Query (\"word1 word2 ...\")\n",
    "               3. Proximity Query (#distance(word1,word2))\n",
    "               4. Combination of the above\n",
    "\n",
    "           Args:\n",
    "               query (string)\n",
    "\n",
    "           Returns:\n",
    "               tokens (list): split the tokens while preserving the Phrasal/Proximity queries\n",
    "        \"\"\"\n",
    "\n",
    "        raw_tokens = query.split(' ')\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(raw_tokens):\n",
    "            rt = raw_tokens[i]\n",
    "            # if a phrasal query\n",
    "            if '\\\"' in rt:\n",
    "                start = i\n",
    "                next_token = ''  # init the next token by assigning an empty string\n",
    "                # stop augmenting i when find the end token (with another \")\n",
    "                while not '\\\"' in next_token:\n",
    "                    i += 1\n",
    "                    next_token = raw_tokens[i]\n",
    "                i += 1\n",
    "                end = i\n",
    "                merged_token = ' '.join(raw_tokens[start: end])\n",
    "                tokens.append(merged_token)\n",
    "            # else if a proximity query\n",
    "            elif '#' in rt:\n",
    "                start = i\n",
    "                next_token = rt  # init the next token by assigning the current token\n",
    "                # stop augmenting i when find the end token (with right bracket)\n",
    "                while not ')' in next_token:\n",
    "                    i += 1\n",
    "                    next_token = raw_tokens[i]\n",
    "                i += 1\n",
    "                end = i\n",
    "                merged_token = ''.join(raw_tokens[start: end])\n",
    "                tokens.append(merged_token)\n",
    "            # else just add the token\n",
    "            else:\n",
    "                tokens.append(rt)\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def preprocess(self, tokens):\n",
    "        \"\"\"apply casefolding & normalization\"\"\" \n",
    "        \n",
    "        return [self.stemmer.stem(t.lower()) for t in tokens]\n",
    "    \n",
    "    \n",
    "    def parse_query(self, query, search='Proximity'):\n",
    "        \"\"\"extract information from the query\"\"\"\n",
    "        \n",
    "        # if proximity query\n",
    "        proximity_parse = re.findall(r'#([0-9]+?)\\((.+?)\\)', query)\n",
    "        if search == 'Proximity':\n",
    "            max_dist = int(proximity_parse[0][0])\n",
    "            terms = self.preprocess(proximity_parse[0][1].split(','))\n",
    "            return terms[0], terms[1], max_dist\n",
    "        \n",
    "        # if phrasal query\n",
    "        phrasal_parse = re.findall(r'\\\"(.+?)\\\"', query)\n",
    "        if search == 'Phrasal':\n",
    "            terms = self.preprocess(phrasal_parse[0].split(' '))\n",
    "            # distance allowed is exactly 1\n",
    "            return terms[0], terms[1], 1\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def linear_merge(self, term1, term2, max_dist, search='Proximity'):\n",
    "        \"\"\"apply linear merge to the two posting lists\"\"\"\n",
    "        \n",
    "        posting_lists = [self.index[term1], self.index[term2]]\n",
    "        docNums = [deque(sorted(posting_lists[0].keys())), \\\n",
    "                   deque(sorted(posting_lists[1].keys()))]\n",
    "        results = []\n",
    "        \n",
    "        # apply linear merge\n",
    "        # init the docNums pointers, cursor points to the current document, ref points the other\n",
    "        cursor = 0\n",
    "        ref = 1\n",
    "        if docNums[1][0] < docNums[0][0]:\n",
    "            cursor = 1\n",
    "            ref = 0\n",
    "        cur_doc_num = docNums[cursor].popleft()\n",
    "        ref_doc_num = docNums[ref].popleft()\n",
    "        # while the deques not empty\n",
    "        while docNums[0] or docNums[1]:\n",
    "            \n",
    "            if cur_doc_num > ref_doc_num:\n",
    "                # swap if current document ID is larger\n",
    "                cursor, ref = ref, cursor\n",
    "                cur_doc_num, ref_doc_num = ref_doc_num, cur_doc_num \n",
    "            \n",
    "            # move the cursor along docNums[cursor] until cur_doc_num >= ref_doc_num\n",
    "            while cur_doc_num < ref_doc_num and docNums[cursor]:\n",
    "                cur_doc_num = docNums[cursor].popleft()\n",
    "            # marginal case: docsNums[cursor] run out but cannot find larger doc_num\n",
    "            if cur_doc_num < ref_doc_num and not docNums[cursor]:\n",
    "                break\n",
    "            \n",
    "            if cur_doc_num == ref_doc_num:\n",
    "                # do the search if find the same doc ID on both sides\n",
    "                left_term_pos = posting_lists[0][cur_doc_num]\n",
    "                right_term_pos = posting_lists[1][cur_doc_num]\n",
    "                # calculate the differences of positions (ref_pos - cur_pos)\n",
    "                dists = [j - i for i in left_term_pos for j in right_term_pos]\n",
    "                # print(dists)\n",
    "                # if Proximity search, order doesn't matter\n",
    "                if search == 'Proximity':\n",
    "                    # using absolute value because order doesn't matter here\n",
    "                    if any([abs(dist) <= max_dist for dist in dists]):\n",
    "                        # print('[Success]: Find a relevant document by proximity [{}] {}'.format(max_dist, cur_doc_num))\n",
    "                        results.append(cur_doc_num)\n",
    "                # else if Phrasal search, order matters\n",
    "                elif search == 'Phrasal':\n",
    "                    # only +1 counts because the order matters\n",
    "                    assert max_dist == 1\n",
    "                    if max_dist in dists:\n",
    "                        # print('[Success]: Find a relevant document with phrase [{}] {}'.format(\" \".join([term1, term2]), cur_doc_num))\n",
    "                        results.append(cur_doc_num)\n",
    "                else:\n",
    "                    print('[Warning!]: Check the search type!')\n",
    "                # move the cursor forward, resulting next iteration must be cur > ref and do the swapping\n",
    "                if docNums[cursor]:\n",
    "                    cur_doc_num = docNums[cursor].popleft()\n",
    "                else:\n",
    "                    # marginal case: if the cursor moves to the end, not necessary to traverse the rest of the refs\n",
    "                    # cause they are always bigger\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def existing(self, *words):\n",
    "        \"\"\"check if all the input words exist in the database\"\"\"\n",
    "        for word in words:\n",
    "            if not word in list(self.index.keys()):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def singleton_search(self, query):\n",
    "        \"\"\"apply single-term/proximity/phrasal search to the input singleton query\n",
    "        \n",
    "           Returns:\n",
    "                 [relevant documents' IDs]\n",
    "        \"\"\"\n",
    "        \n",
    "        results = []\n",
    "        if '#' in query:\n",
    "            term1, term2, max_dist = self.parse_query(query, search='Proximity')\n",
    "            if self.existing(term1, term2):\n",
    "                results = self.linear_merge(term1, term2, max_dist, search='Proximity')\n",
    "        elif '\\\"' in query:\n",
    "            term1, term2, max_dist = self.parse_query(query, search='Phrasal')\n",
    "            if self.existing(term1, term2):\n",
    "                results = self.linear_merge(term1, term2, max_dist, search='Phrasal')\n",
    "        else:\n",
    "            # for a single-term search, just return every document that contains it\n",
    "            term = self.preprocess([query])[0]\n",
    "            if self.existing(term):\n",
    "                results = sorted(list(self.index[term].keys()))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "        \n",
    "    def boolean_search(self, query):\n",
    "        \"\"\"apply general boolean search to the input query \n",
    "        \n",
    "           Returns:\n",
    "                 [relevant documents' IDs]\n",
    "        \"\"\"\n",
    "        \n",
    "        results_stack = []\n",
    "        postfix_queue = deque(Search.shunting_yard(Search.tokenize_query(query)))\n",
    "        # print('Coverse to postfix expression: ', list(postfix_queue))\n",
    "        while postfix_queue:\n",
    "            token = postfix_queue.popleft()\n",
    "            if not token in ['AND', 'OR', 'NOT']:\n",
    "                results_stack.append(self.singleton_search(token))\n",
    "            elif token == 'AND':\n",
    "                right = set(results_stack.pop())\n",
    "                left = set(results_stack.pop())\n",
    "                # left and right\n",
    "                results_stack.append(sorted(list(left.intersection(right))))\n",
    "            elif token == 'OR':\n",
    "                right = set(results_stack.pop())\n",
    "                left = set(results_stack.pop())\n",
    "                # left or right\n",
    "                results_stack.append(sorted(list(left.union(right))))\n",
    "            elif token == 'NOT':\n",
    "                right = set(results_stack.pop())\n",
    "                results_stack.append(sorted(list(self.doc_ids.difference(right))))\n",
    "                \n",
    "        # at this stage, the results_stack should cotain only one list\n",
    "        assert len(results_stack) == 1\n",
    "        return results_stack[0]\n",
    "    \n",
    "    \n",
    "    def boolean_search_multiple(self, query_path):\n",
    "        \"\"\"apply general boolean search to the multiple queries \n",
    "           contained in the input file of the format: \n",
    "               QueryID Query\n",
    "               QueryID Query\n",
    "               ...\n",
    "               \n",
    "           Args:\n",
    "               query_path: the path to the input file containing queries\n",
    "               \n",
    "           Outputs:\n",
    "               results.boolean.txt  \n",
    "        \"\"\"\n",
    "        \n",
    "        with open(query_path, 'r', encoding='utf-8-sig') as f:\n",
    "            queries = f.readlines()\n",
    "            \n",
    "        with open('results.boolean.txt', 'w+', encoding='utf-8') as o:\n",
    "            pa = r'([0-9]+?) (.+?)\\n'  # the regex of each query\n",
    "            # preprocess each query\n",
    "            for query in queries:\n",
    "                queryID, query = re.findall(pa, query)[0]\n",
    "                print('------Processing the query {}------'.format(queryID))\n",
    "                results = self.boolean_search(query)  # a list of relevant document ids\n",
    "                for doc_id in results:\n",
    "                    o.write('{} 0 {} 0 1 0\\n'.format(queryID, doc_id))\n",
    "                    \n",
    "                    \n",
    "    \"\"\" ---------- Below implements the IR by ranking based on TF-IDF ---------- \"\"\"\n",
    "    def _tf(self, term, docID):\n",
    "        \"\"\"calculate the term frequency of a paricular term in a particular document\"\"\"\n",
    "        term = self.stemmer.stem(term.lower())\n",
    "        return len(self.index[term][docID])\n",
    "    \n",
    "    \n",
    "    def _df(self, term):\n",
    "        \"\"\"calculate the document frequency of a particular term\"\"\"\n",
    "        term = self.stemmer.stem(term.lower())\n",
    "        return len(self.index[term].keys())\n",
    "    \n",
    "    \n",
    "    def _weight(self, term, docID):\n",
    "        \"\"\"calculate the weight assigned to a particular term given a particular document:\n",
    "           Formula:\n",
    "                ùë§_{ùë°, ùëë} = (1 + ùëôùëúùëî_10(ùë°ùëì(ùë°, ùëë))) √ó ùëôùëúùëî_10(N / df(t))\n",
    "        \"\"\"\n",
    "        tf = self._tf(term, docID)\n",
    "        df = self._df(term)\n",
    "        N = len(self.doc_ids)\n",
    "        return (1 + np.log10(tf)) * np.log10(N / float(df))\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, query):\n",
    "        \"\"\"apply same tokenization as in building the index\n",
    "           the difference is this time we do not need the stop_word list as we have our index ready\n",
    "        \"\"\"\n",
    "        tokens = self.preprocess(re.findall(r'\\w+', query))\n",
    "        tokens = [token for token in tokens if token in list(self.index.keys())]\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def _score(self, query, docID):\n",
    "        \"\"\"calculate the retrieval score of a query w.r.t a document\n",
    "           Formula:\n",
    "               Score(q, d) = \\sum_(t \\in q and t \\in d) w_{t, d}\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(query)\n",
    "        weights = []\n",
    "        for term in tokens:\n",
    "            # check whether or not the document contains the current term \n",
    "            # otherwise tf will be zero, rendering exception in log10\n",
    "            if docID in self.index[term]:\n",
    "                weights.append(self._weight(term, docID))\n",
    "                \n",
    "        return sum(weights)\n",
    "    \n",
    "    \n",
    "    def _extract_relevant(self, query):\n",
    "        \"\"\"extract relevant documents of terms in the query\"\"\"\n",
    "        tokens = self._tokenize(query)\n",
    "        return list(set().union(*(list(self.index[t].keys()) for t in tokens)))\n",
    "    \n",
    "    \n",
    "    def _ranking(self, query):\n",
    "        \"\"\"give rankings of documents based on Score(q, d)\"\"\"\n",
    "        scores_dict = dict()\n",
    "        # compute the relevant documents by taking the union\n",
    "        for docID in self._extract_relevant(query):\n",
    "            scores_dict[docID] = self._score(query, docID)\n",
    "        return OrderedDict(sorted(scores_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    \n",
    "    \n",
    "    def ranked_retrieval(self, query_path, max_keep=1000):\n",
    "        \"\"\"generate results.ranked.txt\"\"\"\n",
    "        \n",
    "        with open(query_path, 'r', encoding='utf-8-sig') as f:\n",
    "            queries = f.readlines()\n",
    "            \n",
    "        with open('results.ranked.txt', 'w+', encoding='utf-8') as r:\n",
    "            pa = r'([0-9]+?) (.+?)\\n'  # the regex of each query\n",
    "            for query in queries:\n",
    "                queryID, query = re.findall(pa, query)[0]\n",
    "                print('------Generating the rankings for the query {}------'.format(queryID))\n",
    "                print(query)\n",
    "                ranked_dict = self._ranking(query)\n",
    "                count = 0  # trace how many examples kept for each \n",
    "                for docID, score in ranked_dict.items():\n",
    "                    if count < max_keep:\n",
    "                        r.write(\"{} 0 {} 0 {:.4f} 0\\n\".format(queryID, docID, score))\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--index\", type=str, help=\"the path to the binary index\")\n",
    "    parser.add_argument(\"--search\", type=str, help=\"the search type: {bool, rank}\")\n",
    "    parser.add_argument(\"--query\", type=str, help=\"the path to the query file\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    search = Search(args.index, PorterStemmer())\n",
    "    if args.search == 'bool':\n",
    "        search.boolean_search_multiple(args.query)\n",
    "    elif args.search == 'rank':\n",
    "        search.ranked_retrieval(args.query)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
